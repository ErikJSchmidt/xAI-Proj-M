{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor, PILToTensor\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T15:02:16.584900026Z",
     "start_time": "2023-11-21T15:02:14.273479255Z"
    }
   },
   "id": "ae13d8114ff5a644"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d8abd8aa59e7609",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-21T15:02:17.834797517Z",
     "start_time": "2023-11-21T15:02:17.823799269Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = './data/cifar10'\n",
    "train_data_dir = data_dir + \"/train\""
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### First touch with images of cifar10"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f80aa925a6cc3dbc"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folders in cifar10: ['test', 'train']\n",
      "Class folders in cifar10/train ['ship', 'truck', 'horse', 'dog', 'bird', 'deer', 'cat', 'frog', 'automobile', 'airplane']\n"
     ]
    }
   ],
   "source": [
    "print(\"Folders in cifar10:\", os.listdir(data_dir))\n",
    "classes = os.listdir(train_data_dir)\n",
    "print(\"Class folders in cifar10/train\", classes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T15:02:20.084489441Z",
     "start_time": "2023-11-21T15:02:20.077765632Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=32x32>",
      "image/png": "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\n",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1lYUTG8quTgZOMn0qvp+q2F/qep6fGWWfTmVZw4wMEZBHtVu70+z1Sye0vYhJC/UHjB7EHsR6182ax4pvvC3jjXY9NvZ5EctaySTYZ3Qcc/TsetFhH0tYy6fq0TyWF5DcojbGaJwwB9DipZdKYj5cGvnr4W+N49B8RrG282N4ywyr/dPZ/wAM/ka+nB04NJqwHinj2w8dz3ty+kXhbS2ACwwzBHHHPv1968I1Wx1G2vJTeQTiYtlzJyc+pNe3+OdE16K6a+tJJnt8ciJjkfhXmU9zcvMTO7s/Q7+tUM5WymeCcE5APDDpX1N8IvFT6/4ce0urjz7qyYKHY/M0ZHy5+nIrybwzFpuoTrHqFhbTDodyCva/CXhbw/obvfaTZ/Z5pU2vtkJBH0NAH//Z\n"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = Image.open(train_data_dir + \"/airplane/0001.png\")\n",
    "image"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:18:43.275434690Z",
     "start_time": "2023-11-14T10:18:43.173760060Z"
    }
   },
   "id": "f246a3453a968c94"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "pixels = image.getdata()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:27:49.475758857Z",
     "start_time": "2023-11-14T10:27:49.402046116Z"
    }
   },
   "id": "8c0c5665eb35cd9d"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get data gives me a list of 32 * 32 = 1024  pixel tuples, each consisting of three values, e.g.  (202, 204, 199)\n"
     ]
    }
   ],
   "source": [
    "print(\"get data gives me a list of 32 * 32 =\", len(pixels), \" pixel tuples, each consisting of three values, e.g. \", pixels[0])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:30:13.332706382Z",
     "start_time": "2023-11-14T10:30:13.229040422Z"
    }
   },
   "id": "f9683ec2713ac29e"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fits the the image mode of  RGB which needs three colour values between 0 and 255 per pixel.\n"
     ]
    }
   ],
   "source": [
    "print(\"This fits the the image mode of \", image.mode, \"which needs three colour values between 0 and 255 per pixel.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:31:33.413064121Z",
     "start_time": "2023-11-14T10:31:33.263809417Z"
    }
   },
   "id": "af2e07d45b09ffa4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking out ImageFolder loader of torchvision"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0d05e7e763f5f29"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'PIL.Image.Image'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "<PIL.Image.Image image mode=RGB size=32x32>",
      "image/png": "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\n",
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQgJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCAAgACADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwD1lYUTG8quTgZOMn0qvp+q2F/qep6fGWWfTmVZw4wMEZBHtVu70+z1Sye0vYhJC/UHjB7EHsR6182ax4pvvC3jjXY9NvZ5EctaySTYZ3Qcc/TsetFhH0tYy6fq0TyWF5DcojbGaJwwB9DipZdKYj5cGvnr4W+N49B8RrG282N4ywyr/dPZ/wAM/ka+nB04NJqwHinj2w8dz3ty+kXhbS2ACwwzBHHHPv1968I1Wx1G2vJTeQTiYtlzJyc+pNe3+OdE16K6a+tJJnt8ciJjkfhXmU9zcvMTO7s/Q7+tUM5WymeCcE5APDDpX1N8IvFT6/4ce0urjz7qyYKHY/M0ZHy5+nIrybwzFpuoTrHqFhbTDodyCva/CXhbw/obvfaTZ/Z5pU2vtkJBH0NAH//Z\n"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset = ImageFolder(\n",
    "    root=train_data_dir\n",
    ")\n",
    "img, label = train_dataset[0]\n",
    "print(type(img))\n",
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:42:39.016903264Z",
     "start_time": "2023-11-14T10:42:38.577049285Z"
    }
   },
   "id": "b08599f14abb0c39"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[202, 202, 204,  ..., 207, 205, 203],\n         [206, 206, 207,  ..., 210, 208, 206],\n         [210, 211, 212,  ..., 214, 212, 210],\n         ...,\n         [218, 210, 194,  ..., 243, 244, 243],\n         [219, 217, 216,  ..., 241, 241, 241],\n         [217, 216, 217,  ..., 239, 239, 240]],\n\n        [[204, 204, 206,  ..., 208, 206, 204],\n         [208, 208, 209,  ..., 211, 209, 207],\n         [212, 213, 214,  ..., 214, 213, 211],\n         ...,\n         [217, 209, 194,  ..., 242, 242, 243],\n         [218, 216, 216,  ..., 240, 240, 240],\n         [216, 215, 216,  ..., 238, 238, 238]],\n\n        [[199, 199, 201,  ..., 200, 199, 198],\n         [203, 203, 204,  ..., 205, 203, 201],\n         [207, 208, 210,  ..., 210, 208, 206],\n         ...,\n         [222, 214, 198,  ..., 247, 247, 247],\n         [223, 221, 220,  ..., 245, 245, 245],\n         [221, 220, 221,  ..., 243, 243, 243]]], dtype=torch.uint8)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_to_tensor = ImageFolder(\n",
    "    root=train_data_dir,\n",
    "    transform=PILToTensor()\n",
    ")\n",
    "img, label = train_dataset_to_tensor[0]\n",
    "print(type(img))\n",
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:42:40.254572931Z",
     "start_time": "2023-11-14T10:42:39.876100399Z"
    }
   },
   "id": "a05cd69754c358b0"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[0.7922, 0.7922, 0.8000,  ..., 0.8118, 0.8039, 0.7961],\n         [0.8078, 0.8078, 0.8118,  ..., 0.8235, 0.8157, 0.8078],\n         [0.8235, 0.8275, 0.8314,  ..., 0.8392, 0.8314, 0.8235],\n         ...,\n         [0.8549, 0.8235, 0.7608,  ..., 0.9529, 0.9569, 0.9529],\n         [0.8588, 0.8510, 0.8471,  ..., 0.9451, 0.9451, 0.9451],\n         [0.8510, 0.8471, 0.8510,  ..., 0.9373, 0.9373, 0.9412]],\n\n        [[0.8000, 0.8000, 0.8078,  ..., 0.8157, 0.8078, 0.8000],\n         [0.8157, 0.8157, 0.8196,  ..., 0.8275, 0.8196, 0.8118],\n         [0.8314, 0.8353, 0.8392,  ..., 0.8392, 0.8353, 0.8275],\n         ...,\n         [0.8510, 0.8196, 0.7608,  ..., 0.9490, 0.9490, 0.9529],\n         [0.8549, 0.8471, 0.8471,  ..., 0.9412, 0.9412, 0.9412],\n         [0.8471, 0.8431, 0.8471,  ..., 0.9333, 0.9333, 0.9333]],\n\n        [[0.7804, 0.7804, 0.7882,  ..., 0.7843, 0.7804, 0.7765],\n         [0.7961, 0.7961, 0.8000,  ..., 0.8039, 0.7961, 0.7882],\n         [0.8118, 0.8157, 0.8235,  ..., 0.8235, 0.8157, 0.8078],\n         ...,\n         [0.8706, 0.8392, 0.7765,  ..., 0.9686, 0.9686, 0.9686],\n         [0.8745, 0.8667, 0.8627,  ..., 0.9608, 0.9608, 0.9608],\n         [0.8667, 0.8627, 0.8667,  ..., 0.9529, 0.9529, 0.9529]]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_to_tensor_scaled = ImageFolder(\n",
    "    root=train_data_dir,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "img, label = train_dataset_to_tensor_scaled[0]\n",
    "print(type(img))\n",
    "img"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:42:41.962547837Z",
     "start_time": "2023-11-14T10:42:41.639999321Z"
    }
   },
   "id": "76d09fba0f5e9329"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.folder.ImageFolder'>\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))\n",
    "print(len(train_dataset))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T10:50:01.791119840Z",
     "start_time": "2023-11-14T10:50:01.697384195Z"
    }
   },
   "id": "9f778bd27c604b2a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Understanding conv2d layers\n",
    "## Example of a 3 in 1 out size three kernel conv2d\n",
    "Here I manually set the kernel weights of a single convolutional layer that would normally be initialized randomly.\n",
    "The multiplications and additions a conv-layer performs can be observed in this very simple example."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bad45981505a7538"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel dimension: out_channels * in channels * kernel_height * kernel_width torch.Size([1, 3, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "conv1 = nn.Conv2d(3, 1, kernel_size=3, stride=1, padding=0)\n",
    "conv1_kernel = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [0., 0., 0.],\n",
    "                [0., 1., 0.],\n",
    "                [0., 0., 0.]\n",
    "            ],\n",
    "            [\n",
    "                [0., 0., 0.],\n",
    "                [0., 1., 0.],\n",
    "                [0., 0., 0.]\n",
    "            ],\n",
    "            [\n",
    "                [0., 0., 0.],\n",
    "                [0., 1., 0.],\n",
    "                [0., 0., 0.]\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"kernel dimension: out_channels * in channels * kernel_height * kernel_width\", conv1_kernel.shape)\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv1.weight = nn.Parameter(conv1_kernel)\n",
    "\n",
    "simple_model = nn.Sequential(\n",
    "    conv1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:36:49.970805062Z",
     "start_time": "2023-11-14T18:36:49.808776043Z"
    }
   },
   "id": "4f3e81462094f2b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input batch dimensions: batch_size * channels * image_height * image_width =  torch.Size([2, 3, 3, 3])\n",
      "dimension of batch after conv1: batch_size * channels * image_height * image_width =  torch.Size([2, 1, 1, 1])\n",
      "The specified kernel only picks the middle element of each feature map and adds them up over the number of channels.So the result for the first example image id 1*2 + 1*2 + 1*2 + bias = 6 + 0.10041365027427673 = 6.1004133224487305\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([[[[6.1004]]],\n\n\n        [[[3.1004]]]], grad_fn=<ConvolutionBackward0>)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_images = torch.tensor(\n",
    "    [\n",
    "        [\n",
    "            [\n",
    "                [3,2,1],\n",
    "                [3,2,1],\n",
    "                [3,2,1]\n",
    "            ],\n",
    "            [\n",
    "                [3,2,1],\n",
    "                [3,2,1],\n",
    "                [3,2,1]\n",
    "            ],\n",
    "            [\n",
    "                [3,2,1],\n",
    "                [3,2,1],\n",
    "                [3,2,1]\n",
    "            ]\n",
    "        ],\n",
    "        [\n",
    "            [\n",
    "                [1,1,1],\n",
    "                [1,1,1],\n",
    "                [1,1,1]\n",
    "            ],\n",
    "            [\n",
    "                [1,1,1],\n",
    "                [1,1,1],\n",
    "                [1,1,1]\n",
    "            ],\n",
    "            [\n",
    "                [1,1,1],\n",
    "                [1,1,1],\n",
    "                [1,1,1]\n",
    "            ]\n",
    "        ]\n",
    "    ],\n",
    "    dtype=torch.float32)\n",
    "\n",
    "print(\"input batch dimensions: batch_size * channels * image_height * image_width = \", sample_images.shape)\n",
    "sample_batch_output = conv1(sample_images)\n",
    "print(\"dimension of batch after conv1: batch_size * channels * image_height * image_width = \", sample_batch_output.shape)\n",
    "print(\"The specified kernel only picks the middle element of each feature map and adds them up over the number of channels.\"\n",
    "      f\"So the result for the first example image id 1*2 + 1*2 + 1*2 + bias = 6 + {conv1.bias[0]} = {sample_batch_output[0][0][0][0]}\")\n",
    "sample_batch_output"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-14T18:41:38.212643841Z",
     "start_time": "2023-11-14T18:41:38.073302690Z"
    }
   },
   "id": "371133642a3a2b58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "823cc1acf275d46"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
