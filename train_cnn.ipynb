{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:56:13.360493890Z",
     "start_time": "2023-12-01T11:56:11.851210563Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "train_dataset_root_path = \"./data/cifar10/train\"\n",
    "random_seed = 420\n",
    "torch.manual_seed(random_seed)\n",
    "batch_size = 128"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:58:39.306234894Z",
     "start_time": "2023-12-01T11:58:39.253116987Z"
    }
   },
   "id": "9d36c17da68a9673"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading the dataset\n",
    "In the explore_dataset.ipynb we saw that the images of cifar are RGB images, so each image has a dimension of 32 * 32 * 3 (H * W * C).\n",
    "Now we load the the training dataset with help of the ImageFolder data loader, that is specific to dataset folder structures like cifar.\n",
    "By providing ToTensor as transform, images are transformed from PIL.Image.Image objects to tensors [C * H * W] and values are rescaled from [0,255] to [0.0, 1.0] "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1a6547f56f67ba90"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "train_dataset = ImageFolder(\n",
    "    root=train_dataset_root_path,\n",
    "    transform=ToTensor()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:58:40.989879421Z",
     "start_time": "2023-12-01T11:58:40.669021395Z"
    }
   },
   "id": "76a7a10f7c946641"
  },
  {
   "cell_type": "markdown",
   "source": [
    "ImageFolder inherits from VisionDataset, so it should be compatible with further torchvsion functions.  \n",
    "As we can see by inspecting the variable train_dataset, the ImageFolder or VisionDataset does not load all images at once, but creates an index structure over the existing folder structure (a mapping of index to image location on disk)\n",
    "# Splitting the dataset\n",
    "We set aside some of the training samples for validation. Given the VisionDataset the random_split function creates two new objects of type Subset. A subset consists of a reference to the orginal VisinonDataset aka ImageFolder and a sublist of the indices."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a5fd885a45f2288f"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "validation_dataset_size = 5000\n",
    "train_dataset_size = len(train_dataset) - validation_dataset_size\n",
    "\n",
    "train_subset, validation_subset = random_split(train_dataset, [train_dataset_size, validation_dataset_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:58:41.702858003Z",
     "start_time": "2023-12-01T11:58:41.628763878Z"
    }
   },
   "id": "551eb0984969566d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# DataLoaders\n",
    "DataLoaders are yet another layer on top of the Subsets we created. Each dataloader combines a sampling method, e.g. randomly picking samples, with a dataset so that we can easily draw \n",
    "batches of samples."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "330e2b0271a8927f"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "train_data_loader = DataLoader(\n",
    "    dataset=train_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False  # should be True if cuda is available\n",
    ")\n",
    "validation_data_loader = DataLoader(\n",
    "    dataset=validation_subset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=False  # should be True if cuda is available\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:58:42.711430805Z",
     "start_time": "2023-12-01T11:58:42.652998320Z"
    }
   },
   "id": "b9b542828884ef58"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Constructing the CNN Network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a368023ca7868c0"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from models import Cifar10CnnModel, accuracy, evaluate, fit"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:58:43.807535463Z",
     "start_time": "2023-12-01T11:58:43.763093550Z"
    }
   },
   "id": "3197f522c224de63"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "model = Cifar10CnnModel()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:21:43.028515925Z",
     "start_time": "2023-11-30T16:21:42.931848648Z"
    }
   },
   "id": "309d9934d5339972"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "{'val_loss': 2.3028876781463623, 'val_acc': 0.10051688551902771}"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, train_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T16:25:25.142574706Z",
     "start_time": "2023-11-30T16:21:43.454464717Z"
    }
   },
   "id": "93014a3adfddb5ee"
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> without training 0.10051 accuracy makes sense with 10 classes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "82ac0e36679e6121"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "opt_func = torch.optim.Adam\n",
    "lr = 0.001"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T17:46:04.277909005Z",
     "start_time": "2023-11-30T17:46:04.226011227Z"
    }
   },
   "id": "6eb332228cf3f1d4"
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note**: I do not have CUDA available locally, so up to here nothing is done to shift load to the GPU. Should move to Colab and make use of GPU as training seems not feasible with my laptop CPU... (after 41 minutes not one of 10 epochs is done)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "19a226387290cf24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = fit(num_epochs, lr, model, train_data_loader, validation_data_loader, opt_func)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-14T21:41:03.338115143Z"
    }
   },
   "id": "7e961e6f942b6cfb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Plain18Layer"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b93e18f7faa4d77d"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import custom_cnn.models as models\n",
    "from importlib import reload\n",
    "reload(models)\n",
    "pl18_model = models.Plain18Layer()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T17:45:43.910415257Z",
     "start_time": "2023-11-30T17:45:43.784888678Z"
    }
   },
   "id": "60b265d1493cfe15"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "{'val_loss': 2.3025715351104736, 'val_acc': 0.10047468543052673}"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(pl18_model, validation_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T17:45:54.885225975Z",
     "start_time": "2023-11-30T17:45:45.672687226Z"
    }
   },
   "id": "a019b1ab28256111"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "history = fit(num_epochs, lr, model, train_data_loader, validation_data_loader, opt_func)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-11-30T17:46:06.828893890Z"
    }
   },
   "id": "256fc542efdc33c2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load a model saved after traning in colab"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3c5219e3def68d9c"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[13], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m Cifar10CnnModel()\n\u001B[1;32m      4\u001B[0m model\u001B[38;5;241m.\u001B[39mnetwork\u001B[38;5;241m.\u001B[39mload_state_dict(torch\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcustom_cnn/savedmodels/Cifar10CnnModel_20231128_12:26\u001B[39m\u001B[38;5;124m\"\u001B[39m, map_location\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)))\n\u001B[0;32m----> 6\u001B[0m \u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalidation_data_loader\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/WiSe2023_24/xAI-Proj-M/xAI-Proj-M/models.py:157\u001B[0m, in \u001B[0;36mevaluate\u001B[0;34m(model, val_loader)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(model, val_loader):\n\u001B[1;32m    156\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m--> 157\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [model\u001B[38;5;241m.\u001B[39mvalidation_step(batch) \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m val_loader]\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mvalidation_epoch_end(outputs)\n",
      "File \u001B[0;32m~/Desktop/WiSe2023_24/xAI-Proj-M/xAI-Proj-M/models.py:157\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mno_grad()\n\u001B[1;32m    155\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mevaluate\u001B[39m(model, val_loader):\n\u001B[1;32m    156\u001B[0m     model\u001B[38;5;241m.\u001B[39meval()\n\u001B[0;32m--> 157\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m [\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvalidation_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m batch \u001B[38;5;129;01min\u001B[39;00m val_loader]\n\u001B[1;32m    158\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model\u001B[38;5;241m.\u001B[39mvalidation_epoch_end(outputs)\n",
      "File \u001B[0;32m~/Desktop/WiSe2023_24/xAI-Proj-M/xAI-Proj-M/models.py:23\u001B[0m, in \u001B[0;36mImageClassificationBase.validation_step\u001B[0;34m(self, batch)\u001B[0m\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mvalidation_step\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch):\n\u001B[1;32m     22\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m batch\n\u001B[0;32m---> 23\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m                    \u001B[38;5;66;03m# Generate predictions\u001B[39;00m\n\u001B[1;32m     24\u001B[0m     loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(out, labels)   \u001B[38;5;66;03m# Calculate loss\u001B[39;00m\n\u001B[1;32m     25\u001B[0m     acc \u001B[38;5;241m=\u001B[39m accuracy(out, labels)           \u001B[38;5;66;03m# Calculate accuracy\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Desktop/WiSe2023_24/xAI-Proj-M/xAI-Proj-M/models.py:74\u001B[0m, in \u001B[0;36mCifar10CnnModel.forward\u001B[0;34m(self, xb)\u001B[0m\n\u001B[1;32m     73\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, xb):\n\u001B[0;32m---> 74\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[43m(\u001B[49m\u001B[43mxb\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from models import Cifar10CnnModel\n",
    "\n",
    "model = Cifar10CnnModel()\n",
    "model.network.load_state_dict(torch.load(\"custom_cnn/savedmodels/Cifar10CnnModel_20231128_12:26\", map_location=torch.device('cpu')))\n",
    "\n",
    "evaluate(model, validation_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:59:01.922313959Z",
     "start_time": "2023-12-01T11:58:53.021052889Z"
    }
   },
   "id": "f8530826035b36a5"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"4.weight\", \"13.weight\", \"22.weight\", \"31.weight\", \"38.bias\", \"40.weight\", \"40.bias\". \n\tUnexpected key(s) in state_dict: \"44.weight\", \"45.weight\", \"45.bias\", \"45.running_mean\", \"45.running_var\", \"45.num_batches_tracked\", \"47.weight\", \"48.weight\", \"48.bias\", \"48.running_mean\", \"48.running_var\", \"48.num_batches_tracked\", \"52.weight\", \"52.bias\", \"3.weight\", \"3.bias\", \"3.running_mean\", \"3.running_var\", \"3.num_batches_tracked\", \"5.weight\", \"6.bias\", \"6.running_mean\", \"6.running_var\", \"6.num_batches_tracked\", \"9.weight\", \"9.bias\", \"9.running_mean\", \"9.running_var\", \"9.num_batches_tracked\", \"12.weight\", \"12.bias\", \"12.running_mean\", \"12.running_var\", \"12.num_batches_tracked\", \"14.weight\", \"15.bias\", \"15.running_mean\", \"15.running_var\", \"15.num_batches_tracked\", \"18.weight\", \"18.bias\", \"18.running_mean\", \"18.running_var\", \"18.num_batches_tracked\", \"21.weight\", \"21.bias\", \"21.running_mean\", \"21.running_var\", \"21.num_batches_tracked\", \"23.weight\", \"24.bias\", \"24.running_mean\", \"24.running_var\", \"24.num_batches_tracked\", \"27.weight\", \"27.bias\", \"27.running_mean\", \"27.running_var\", \"27.num_batches_tracked\", \"30.weight\", \"30.bias\", \"30.running_mean\", \"30.running_var\", \"30.num_batches_tracked\", \"32.weight\", \"33.bias\", \"33.running_mean\", \"33.running_var\", \"33.num_batches_tracked\", \"36.weight\", \"36.bias\", \"36.running_mean\", \"36.running_var\", \"36.num_batches_tracked\", \"39.weight\", \"39.bias\", \"39.running_mean\", \"39.running_var\", \"39.num_batches_tracked\", \"41.weight\", \"42.running_mean\", \"42.running_var\", \"42.num_batches_tracked\". \n\tsize mismatch for 6.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for 11.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for 15.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for 20.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for 24.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for 26.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for 29.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for 33.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for 35.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for 38.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).\n\tsize mismatch for 42.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for 42.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([10]).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[14], line 4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmodels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Plain18Layer\n\u001B[1;32m      3\u001B[0m model \u001B[38;5;241m=\u001B[39m Plain18Layer()\n\u001B[0;32m----> 4\u001B[0m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnetwork\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_state_dict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcustom_cnn/savedmodels/Plain18Layer20231130_19:29\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmap_location\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcpu\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      6\u001B[0m evaluate(model, validation_data_loader)\n",
      "File \u001B[0;32m~/anaconda3/envs/xAI-Proj-M/lib/python3.8/site-packages/torch/nn/modules/module.py:2152\u001B[0m, in \u001B[0;36mModule.load_state_dict\u001B[0;34m(self, state_dict, strict, assign)\u001B[0m\n\u001B[1;32m   2147\u001B[0m         error_msgs\u001B[38;5;241m.\u001B[39minsert(\n\u001B[1;32m   2148\u001B[0m             \u001B[38;5;241m0\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mMissing key(s) in state_dict: \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2149\u001B[0m                 \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mk\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m missing_keys)))\n\u001B[1;32m   2151\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(error_msgs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m-> 2152\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mError(s) in loading state_dict for \u001B[39m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\n\u001B[1;32m   2153\u001B[0m                        \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mjoin(error_msgs)))\n\u001B[1;32m   2154\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Error(s) in loading state_dict for Sequential:\n\tMissing key(s) in state_dict: \"4.weight\", \"13.weight\", \"22.weight\", \"31.weight\", \"38.bias\", \"40.weight\", \"40.bias\". \n\tUnexpected key(s) in state_dict: \"44.weight\", \"45.weight\", \"45.bias\", \"45.running_mean\", \"45.running_var\", \"45.num_batches_tracked\", \"47.weight\", \"48.weight\", \"48.bias\", \"48.running_mean\", \"48.running_var\", \"48.num_batches_tracked\", \"52.weight\", \"52.bias\", \"3.weight\", \"3.bias\", \"3.running_mean\", \"3.running_var\", \"3.num_batches_tracked\", \"5.weight\", \"6.bias\", \"6.running_mean\", \"6.running_var\", \"6.num_batches_tracked\", \"9.weight\", \"9.bias\", \"9.running_mean\", \"9.running_var\", \"9.num_batches_tracked\", \"12.weight\", \"12.bias\", \"12.running_mean\", \"12.running_var\", \"12.num_batches_tracked\", \"14.weight\", \"15.bias\", \"15.running_mean\", \"15.running_var\", \"15.num_batches_tracked\", \"18.weight\", \"18.bias\", \"18.running_mean\", \"18.running_var\", \"18.num_batches_tracked\", \"21.weight\", \"21.bias\", \"21.running_mean\", \"21.running_var\", \"21.num_batches_tracked\", \"23.weight\", \"24.bias\", \"24.running_mean\", \"24.running_var\", \"24.num_batches_tracked\", \"27.weight\", \"27.bias\", \"27.running_mean\", \"27.running_var\", \"27.num_batches_tracked\", \"30.weight\", \"30.bias\", \"30.running_mean\", \"30.running_var\", \"30.num_batches_tracked\", \"32.weight\", \"33.bias\", \"33.running_mean\", \"33.running_var\", \"33.num_batches_tracked\", \"36.weight\", \"36.bias\", \"36.running_mean\", \"36.running_var\", \"36.num_batches_tracked\", \"39.weight\", \"39.bias\", \"39.running_mean\", \"39.running_var\", \"39.num_batches_tracked\", \"41.weight\", \"42.running_mean\", \"42.running_var\", \"42.num_batches_tracked\". \n\tsize mismatch for 6.weight: copying a param with shape torch.Size([16]) from checkpoint, the shape in current model is torch.Size([16, 16, 3, 3]).\n\tsize mismatch for 11.weight: copying a param with shape torch.Size([16, 16, 3, 3]) from checkpoint, the shape in current model is torch.Size([32, 16, 3, 3]).\n\tsize mismatch for 15.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([32, 32, 3, 3]).\n\tsize mismatch for 20.weight: copying a param with shape torch.Size([32, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 32, 3, 3]).\n\tsize mismatch for 24.weight: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for 26.weight: copying a param with shape torch.Size([64, 32, 3, 3]) from checkpoint, the shape in current model is torch.Size([64, 64, 3, 3]).\n\tsize mismatch for 29.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 64, 3, 3]).\n\tsize mismatch for 33.weight: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for 35.weight: copying a param with shape torch.Size([64, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([128, 128, 3, 3]).\n\tsize mismatch for 38.weight: copying a param with shape torch.Size([128, 64, 3, 3]) from checkpoint, the shape in current model is torch.Size([1024, 2048]).\n\tsize mismatch for 42.weight: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([10, 512]).\n\tsize mismatch for 42.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([10])."
     ]
    }
   ],
   "source": [
    "from models import Plain18Layer\n",
    "\n",
    "model = Plain18Layer()\n",
    "model.network.load_state_dict(torch.load(\"custom_cnn/savedmodels/Plain18Layer20231130_19:29\", map_location=torch.device('cpu')))\n",
    "\n",
    "evaluate(model, validation_data_loader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-01T11:59:04.334058915Z",
     "start_time": "2023-12-01T11:59:04.147256552Z"
    }
   },
   "id": "fe0d9dd2af8d803f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "=> must use the pretrained params, but values are slightly of. Is this due to different machine or GPU for training and CPU for evaluation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "521d8434c029015f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
