{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from knn_loss import KNNLoss\n",
    "import utility_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regen_rand():\n",
    "    # TODO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_a = torch.tensor([[2.0], [3.0], [4.0]])\n",
    "tensor_b = torch.tensor([[0.0], [0.0], [0.0]])\n",
    "tensor_c = torch.tensor([[-2.0], [-3.0], [-4.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.],\n",
       "        [0.],\n",
       "        [0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors = [tensor_a, tensor_b, tensor_c]\n",
    "stack = torch.stack(tensors)\n",
    "torch.mean(stack, dim = 0)\n",
    "# stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forward_pass = torch.randn(256, 128, 1)\n",
    "labels = torch.rand(256, 1, 1)\n",
    "labels *= 9\n",
    "labels = torch.absolute(labels)\n",
    "labels = torch.round(labels)\n",
    "\n",
    "classes = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "classes_tensor = torch.tensor(classes)\n",
    "classes_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrap = KNNLoss(classes_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrap = KNNLoss(classes_tensor)\n",
    "# div_loss = wrap.divergence_loss(forward_pass, labels)\n",
    "# print(div_loss)\n",
    "# centroids = wrap.get_centroids()\n",
    "# print(centroids.shape)\n",
    "# conv_loss = wrap.convergence_loss(forward_pass, labels)\n",
    "# print(conv_loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the `# Calculate Current Centroids` section seem to work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(forward_pass[0])\n",
    "# print(forward_pass[1])\n",
    "\n",
    "# a = forward_pass[0]\n",
    "# b = forward_pass[1]\n",
    "\n",
    "# cos = torch.nn.CosineSimilarity(dim = 0)\n",
    "# wrap.cosine_similarity(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(wrap.get_centroids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_pass = utility_functions.to_device(torch.rand(256, 128, requires_grad=True), utility_functions.get_default_device())\n",
    "labels = utility_functions.to_device(torch.rand(256, 1, requires_grad=True), utility_functions.get_default_device()).mul(9).absolute().round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9., device='cuda:0', grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "print(labels.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0661, -1.9436,  0.2760,  ...,  0.0201, -0.2203,  0.6937],\n",
      "        [-0.9049, -0.5431,  2.0047,  ...,  0.9057, -0.1693, -1.0594],\n",
      "        [-1.0525, -0.5521,  1.3504,  ...,  0.4818,  0.2720, -1.7054],\n",
      "        ...,\n",
      "        [-0.7704,  0.7372, -0.0790,  ..., -0.8952, -0.4362, -2.6234],\n",
      "        [ 0.6190,  1.7805, -0.4956,  ...,  0.6191, -1.9304,  2.5194],\n",
      "        [ 0.7160,  0.0582,  0.0110,  ...,  0.5262, -0.5016, -0.1412]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "debug_1 = utility_functions.to_device(torch.randn(len(classes_tensor), forward_pass.shape[1]), wrap.device)\n",
    "print(debug_1)\n",
    "wrap.centroids = debug_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3960, 0.3989, 0.3826,  ..., 0.5575, 0.5347, 0.6373],\n",
      "        [0.5356, 0.4312, 0.5038,  ..., 0.4464, 0.5068, 0.3853],\n",
      "        [0.4869, 0.4462, 0.5584,  ..., 0.5714, 0.3509, 0.3909],\n",
      "        ...,\n",
      "        [0.5254, 0.4863, 0.3490,  ..., 0.5248, 0.4165, 0.3545],\n",
      "        [0.4607, 0.5573, 0.4094,  ..., 0.4609, 0.4275, 0.5645],\n",
      "        [0.7235, 0.3993, 0.4834,  ..., 0.6116, 0.4802, 0.5535]],\n",
      "       device='cuda:0', grad_fn=<StackBackward0>)\n"
     ]
    }
   ],
   "source": [
    "c = []\n",
    "for _c in wrap.classes:\n",
    "    class_l = [wrap.centroids[_c].detach()]\n",
    "    for inp, tar in zip(forward_pass, labels):\n",
    "        if int(tar) == _c:\n",
    "            class_l.append(inp)\n",
    "    stacked_t = torch.stack(class_l)\n",
    "    c.append(torch.mean(stacked_t, dim = 0))\n",
    "wrap.centroids = torch.stack(c)\n",
    "print(wrap.centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9758, device='cuda:0', grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "for centroid_a in wrap.centroids:\n",
    "    local_d = []    # for each centroid collect distances to other centroids in a list. This list should be ALWAYS be 9 elements long\n",
    "    for centroid_b in wrap.centroids:\n",
    "        if not torch.equal(centroid_a, centroid_b):\n",
    "            local_d.append(wrap.cosine_similarity(centroid_a, centroid_b))\n",
    "    local_t = torch.stack(local_d)\n",
    "    d.append(torch.mean(local_t))\n",
    "wrap.mean_centroid_distance = torch.stack(d).abs().mean()\n",
    "print(wrap.mean_centroid_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_d = []\n",
    "for inp, tar in zip(forward_pass, labels):\n",
    "    label = int(tar)\n",
    "    corresponding_centroid = wrap.centroids[label].detach()\n",
    "    instance_d.append(wrap.euclidean_distance(inp, corresponding_centroid))\n",
    "d = torch.mul(torch.stack(instance_d), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "convergence = torch.mean(d)\n",
    "uniformity = torch.std(d)\n",
    "divergence = torch.div(1, torch.sub(1, wrap.mean_centroid_distance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.2372, device='cuda:0', grad_fn=<MeanBackward0>) tensor(0.1469, device='cuda:0', grad_fn=<StdBackward0>) tensor(41.2923, device='cuda:0', grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    convergence,\n",
    "    uniformity,\n",
    "    divergence\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> _Now the Ratio doesn't make sense anymore..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(139.7388, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(torch.mul(divergence, convergence.add(uniformity)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [utility_functions.to_device(torch.rand(256, 128, requires_grad=True), utility_functions.get_default_device()) for i in range(195)]\n",
    "targets = [utility_functions.to_device(torch.rand(256, 1, requires_grad=True), utility_functions.get_default_device()).mul(9).absolute().round() for i in range(195)]\n",
    "\n",
    "loss = KNNLoss(classes_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [utility_functions.to_device(torch.rand(256, 128, requires_grad=True), utility_functions.get_default_device()) for i in range(64)]\n",
    "targets = [utility_functions.to_device(torch.rand(256, 1, requires_grad=True), utility_functions.get_default_device()).mul(9).absolute().round() for i in range(64)]\n",
    "\n",
    "loss = KNNLoss(classes_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(150.8646, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(258.3460, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(266.7856, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.2249, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(262.8551, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.3738, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(211.2651, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(268.9648, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(271.2145, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.2696, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(241.6110, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(233.6987, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.2541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(240.5599, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(271.3982, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(242.3234, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.6206, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(234.3523, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.0891, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(250.3980, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(234.3209, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.6277, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(267.2214, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(273.4542, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(242.1676, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.2816, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.7219, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.4740, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(272.4354, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.8050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.6593, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.0550, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(288.1722, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(259.6769, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(227.1111, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(269.4444, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(241.2557, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(271.7930, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.9411, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.0254, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.0930, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(236.6519, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(282.6784, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.3994, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(261.9017, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(262.7926, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(271.2286, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.1988, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(265.3484, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(272.5384, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(245.4662, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(249.9955, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.5672, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(229.2356, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(250.7621, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(248.6459, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.7268, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(277.2050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.7408, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(277.9267, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(268.6554, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(245.9568, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(239.6337, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.1929, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(243.7313, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(262.1920, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.3049, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(265.8401, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.3594, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.6128, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(292.4465, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(273.5089, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(238.3883, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(253.1113, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(222.8164, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(244.8017, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(245.9377, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.3210, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(269.7207, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(267.6785, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(195.0054, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(268.7036, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.0284, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(233.4885, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(262.6683, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(268.6103, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(263.0420, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(275.3948, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.0469, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(266.9171, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(266.2792, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(243.9453, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.0444, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(258.5110, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.6958, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(277.7895, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.9395, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.9290, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(254.9624, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(242.4961, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(278.3860, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(228.2441, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(259.1949, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(235.7132, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(228.7204, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.0209, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(272.1733, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(261.6897, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.0025, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(274.1541, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(254.2440, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(247.9313, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(288.3942, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(269.5630, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(250.8669, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(227.8525, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(265.1317, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(228.1457, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(245.1816, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(254.2050, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(236.6269, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(239.1055, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.9297, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(264.0283, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(264.8763, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.8887, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(258.0002, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(249.2463, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(241.4866, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(238.1118, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.9649, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(290.5914, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(256.2428, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(276.2451, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.5384, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(237.4587, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(274.0390, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(237.0571, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(226.6257, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(264.8054, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(261.4492, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(249.1310, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(268.1276, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(227.0118, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(249.1769, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(259.1826, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(222.6158, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(208.0154, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(258.9212, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(246.6107, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(241.5946, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.1470, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(288.9112, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(272.2290, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.8387, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.5652, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(259.4794, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(269.5077, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(276.3306, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(236.4630, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.3259, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.7549, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(239.1548, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(253.9838, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(267.9244, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(280.7556, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(262.9770, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.0697, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(277.9608, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(279.0387, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(252.2699, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(260.4079, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(263.4498, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(261.4526, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(281.5619, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(250.8438, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(255.4672, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(243.2302, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(250.0785, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(254.6797, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(242.8963, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(251.1016, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(264.9512, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(276.3003, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(219.4241, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(270.4526, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.5839, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(254.3804, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(241.4558, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(252.4020, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(257.7973, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(259.7909, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(269.2069, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(222.4598, device='cuda:0', grad_fn=<MulBackward0>)\n",
      "tensor(258.8694, device='cuda:0', grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for input, target in zip(inputs, targets):\n",
    "    print(\n",
    "        loss.loss_renewed(input, target)\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
